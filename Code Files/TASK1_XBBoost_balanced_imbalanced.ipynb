{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "sHzCO7o3o52i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "SZLJSu1jhtbC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('/content/drive/MyDrive/NLPFinalProject/Model Implementation/balancedtask1.csv')\n",
        "data.head()\n",
        "\n",
        "# Preprocess the text data\n",
        "# For example, using the NLTK library:\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['chat1_processed'] = data['chat1'].apply(preprocess_text)\n",
        "data['chat2_processed'] = data['chat2'].apply(preprocess_text)\n",
        "\n",
        "# Extract features from the preprocessed text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['chat1_processed'] + ' ' + data['chat2_processed'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a SVM model on the training set\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Precision:', precision_score(y_test, y_pred))\n",
        "print('Recall:', recall_score(y_test, y_pred))\n",
        "print('F1 score:', f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOOlyRB1o55B",
        "outputId": "6d5fe719-f178-426b-86a9-2d5fe539812d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7818181818181819\n",
            "Precision: 0.8164251207729468\n",
            "Recall: 0.7444933920704846\n",
            "F1 score: 0.7788018433179724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "0GCbsmm_hWoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e66c05-8a20-497e-e86f-52457b230329"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False       0.75      0.82      0.78       213\n",
            "        True       0.82      0.74      0.78       227\n",
            "\n",
            "    accuracy                           0.78       440\n",
            "   macro avg       0.78      0.78      0.78       440\n",
            "weighted avg       0.78      0.78      0.78       440\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {'C': [0.1, 1], \n",
        "              'gamma': [1, 0.1],\n",
        "              'kernel': ['linear', 'rbf', 'poly']}\n",
        "\n",
        "# Run the grid search\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Find the best parameters\n",
        "print(grid.best_params_)\n",
        "\n",
        "# Train a SVM model on the training set with the best parameters\n",
        "svm_best = SVC(kernel=grid.best_params_['kernel'], C=grid.best_params_['C'], gamma=grid.best_params_['gamma'])\n",
        "svm_best.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_best = svm_best.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred_best))\n",
        "print('Precision:', precision_score(y_test, y_pred_best))\n",
        "print('Recall:', recall_score(y_test, y_pred_best))\n",
        "print('F1 score:', f1_score(y_test, y_pred_best))\n"
      ],
      "metadata": {
        "id": "F1nvsioK8Km1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d38404f-ae8b-44ed-f91e-919a4beb0028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   7.3s\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   6.8s\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   7.4s\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   7.1s\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   7.6s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.6s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.7s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.1s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   7.8s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   6.8s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   7.4s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   6.6s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   7.3s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   6.9s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   7.4s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   6.8s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   7.5s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   6.6s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   7.3s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   6.7s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   7.3s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   6.6s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   7.3s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   6.8s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   7.2s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   6.6s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   7.2s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   6.7s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   6.9s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   5.9s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   6.3s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   6.2s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   5.8s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   7.1s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   6.5s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   7.3s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   6.5s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   7.3s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   6.6s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   7.3s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   6.7s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   7.2s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   6.6s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   6.8s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   6.0s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   6.5s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   5.9s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   6.1s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   6.8s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   6.9s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   6.9s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   6.8s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   7.0s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   6.7s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   7.0s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   6.6s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   7.0s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   6.5s\n",
            "{'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
            "Accuracy: 0.7931818181818182\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.748898678414097\n",
            "F1 score: 0.7888631090487239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))"
      ],
      "metadata": {
        "id": "wQMZP6tXhaSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iZ8af9bSH-A",
        "outputId": "51e07932-f3c0-4ffb-ff6d-fd672e0214ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Lemmatization instead of Stemming\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text_lemmatize(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['chat1_processed'] = data['chat1'].apply(preprocess_text_lemmatize)\n",
        "data['chat2_processed'] = data['chat2'].apply(preprocess_text_lemmatize)\n",
        "\n",
        "# Using a more complex model like XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Extract features from the preprocessed text data\n",
        "X = vectorizer.fit_transform(data['chat1_processed'] + ' ' + data['chat2_processed'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a XGBClassifier model on the training set\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred_xgb))\n",
        "print('Precision:', precision_score(y_test, y_pred_xgb))\n",
        "print('Recall:', recall_score(y_test, y_pred_xgb))\n",
        "print('F1 score:', f1_score(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULXmVSjyXaTq",
        "outputId": "0340974e-33e0-4ae7-97c7-36deff95bcc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8181818181818182\n",
            "Precision: 0.8450704225352113\n",
            "Recall: 0.7929515418502202\n",
            "F1 score: 0.8181818181818181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "kG34yuf7aDLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Lemmatization instead of Stemming\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text_lemmatize(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['chat1_processed'] = data['chat1'].apply(preprocess_text_lemmatize)\n",
        "data['chat2_processed'] = data['chat2'].apply(preprocess_text_lemmatize)\n",
        "\n",
        "# Using a more complex model like XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Extract features from the preprocessed text data\n",
        "X = vectorizer.fit_transform(data['chat1_processed'] + ' ' + data['chat2_processed'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a XGBClassifier model on the training set\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred_xgb))\n",
        "print('Precision:', precision_score(y_test, y_pred_xgb))\n",
        "print('Recall:', recall_score(y_test, y_pred_xgb))\n",
        "print('F1 score:', f1_score(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHua6mQpaviR",
        "outputId": "b51a907b-2f45-4694-8d51-a0837859c30f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8340909090909091\n",
            "Precision: 0.8289473684210527\n",
            "Recall: 0.5121951219512195\n",
            "F1 score: 0.6331658291457286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred_xgb))\n"
      ],
      "metadata": {
        "id": "9_xahGPqhe-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/NLPFinalProject/Model Implementation/imbalancedtask1.csv')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text_lemmatize(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['chat1_processed'] = data['chat1'].apply(preprocess_text_lemmatize)\n",
        "data['chat2_processed'] = data['chat2'].apply(preprocess_text_lemmatize)\n",
        "\n",
        "# Using a more complex model like XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Extract features from the preprocessed text data\n",
        "X = vectorizer.fit_transform(data['chat1_processed'] + ' ' + data['chat2_processed'])"
      ],
      "metadata": {
        "id": "-kPFrfGxhfLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1acbaa1-b315-4b90-e62d-6ba99eb11ccc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SMOTE for oversampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Perform oversampling before splitting into training and test sets\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, data['label'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'subsample': [0.5,],\n",
        "    'colsample_bytree': [0.5],\n",
        "    'n_estimators' : [100],\n",
        "    'objective': ['binary:logistic']\n",
        "}\n",
        "\n",
        "# Initialize the classifier\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring='f1', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters: \", best_params)\n",
        "\n",
        "# Train a XGBClassifier model on the training set with the best parameters\n",
        "xgb_best = XGBClassifier(**best_params)\n",
        "xgb_best.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb = xgb_best.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred_xgb))\n",
        "print('Precision:', precision_score(y_test, y_pred_xgb))\n",
        "print('Recall:', recall_score(y_test, y_pred_xgb))\n",
        "print('F1 score:', f1_score(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks3-2km7b4ra",
        "outputId": "1444fdeb-3d0b-4f03-fa1a-17be1eb221c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best parameters:  {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'objective': 'binary:logistic', 'subsample': 0.5}\n",
            "Accuracy: 0.8924242424242425\n",
            "Precision: 0.922360248447205\n",
            "Recall: 0.8658892128279884\n",
            "F1 score: 0.8932330827067669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFsLKxrQhgQR",
        "outputId": "90295b67-d953-4405-d5ec-8e858986ccc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False       0.86      0.92      0.89       317\n",
            "        True       0.92      0.87      0.89       343\n",
            "\n",
            "    accuracy                           0.89       660\n",
            "   macro avg       0.89      0.89      0.89       660\n",
            "weighted avg       0.89      0.89      0.89       660\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOnU8KDrhhBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}