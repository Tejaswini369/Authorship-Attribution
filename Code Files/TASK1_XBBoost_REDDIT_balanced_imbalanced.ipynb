{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "sHzCO7o3o52i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "SZLJSu1jhtbC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('/content/drive/MyDrive/NLPFinalProject/Model Implementation/balancedtask1reddit.csv')\n",
        "data.head()\n",
        "\n",
        "# Preprocess the text data\n",
        "# For example, using the NLTK library:\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "data['chat1_processed'] = data['chat1'].apply(preprocess_text)\n",
        "data['chat2_processed'] = data['chat2'].apply(preprocess_text)\n",
        "\n",
        "# Extract features from the preprocessed text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['chat1_processed'] + ' ' + data['chat2_processed'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a SVM model on the training set\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Precision:', precision_score(y_test, y_pred))\n",
        "print('Recall:', recall_score(y_test, y_pred))\n",
        "print('F1 score:', f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOOlyRB1o55B",
        "outputId": "0d8f37bf-33d4-4cea-b894-75cfbbab47eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.725\n",
            "Precision: 0.7735849056603774\n",
            "Recall: 0.6612903225806451\n",
            "F1 score: 0.7130434782608696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVK5PHQ0qKEh",
        "outputId": "cefc34ff-69e5-49cb-a14d-a5ca1a7bb08d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     300\n",
              "False    300\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "0GCbsmm_hWoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e66c05-8a20-497e-e86f-52457b230329"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False       0.75      0.82      0.78       213\n",
            "        True       0.82      0.74      0.78       227\n",
            "\n",
            "    accuracy                           0.78       440\n",
            "   macro avg       0.78      0.78      0.78       440\n",
            "weighted avg       0.78      0.78      0.78       440\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {'C': [0.1, 1], \n",
        "              'gamma': [1, 0.1],\n",
        "              'kernel': ['linear', 'rbf', 'poly']}\n",
        "\n",
        "# Run the grid search\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Find the best parameters\n",
        "print(grid.best_params_)\n",
        "\n",
        "# Train a SVM model on the training set with the best parameters\n",
        "svm_best = SVC(kernel=grid.best_params_['kernel'], C=grid.best_params_['C'], gamma=grid.best_params_['gamma'])\n",
        "svm_best.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_best = svm_best.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred_best))\n",
        "print('Precision:', precision_score(y_test, y_pred_best))\n",
        "print('Recall:', recall_score(y_test, y_pred_best))\n",
        "print('F1 score:', f1_score(y_test, y_pred_best))\n"
      ],
      "metadata": {
        "id": "F1nvsioK8Km1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d38404f-ae8b-44ed-f91e-919a4beb0028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   7.3s\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   6.8s\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   7.4s\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   7.1s\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=   7.6s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.6s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.0s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.7s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   7.1s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   7.8s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   6.8s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   7.4s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   6.6s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   7.3s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   6.9s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   7.4s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   6.8s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   7.5s\n",
            "[CV] END ....................C=0.1, gamma=0.1, kernel=linear; total time=   6.6s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   7.3s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   6.7s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   7.3s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   6.6s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   7.3s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   6.8s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   7.2s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   6.6s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   7.2s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   6.7s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   6.9s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   5.9s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   6.3s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   6.2s\n",
            "[CV] END ........................C=1, gamma=1, kernel=linear; total time=   5.8s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   7.1s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   6.5s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   7.3s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   6.5s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   7.3s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   6.6s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   7.3s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   6.7s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   7.2s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   6.6s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   6.8s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   6.0s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   6.5s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   5.9s\n",
            "[CV] END ......................C=1, gamma=0.1, kernel=linear; total time=   6.1s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   6.8s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   6.9s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   6.9s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   6.8s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   7.0s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   6.7s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   7.0s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   6.6s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   7.0s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   6.5s\n",
            "{'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
            "Accuracy: 0.7931818181818182\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.748898678414097\n",
            "F1 score: 0.7888631090487239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))"
      ],
      "metadata": {
        "id": "wQMZP6tXhaSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iZ8af9bSH-A",
        "outputId": "51e07932-f3c0-4ffb-ff6d-fd672e0214ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Lemmatization instead of Stemming\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text_lemmatize(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['chat1_processed'] = data['chat1'].apply(preprocess_text_lemmatize)\n",
        "data['chat2_processed'] = data['chat2'].apply(preprocess_text_lemmatize)\n",
        "\n",
        "# Using a more complex model like XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Extract features from the preprocessed text data\n",
        "X = vectorizer.fit_transform(data['chat1_processed'] + ' ' + data['chat2_processed'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a XGBClassifier model on the training set\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred_xgb))\n",
        "print('Precision:', precision_score(y_test, y_pred_xgb))\n",
        "print('Recall:', recall_score(y_test, y_pred_xgb))\n",
        "print('F1 score:', f1_score(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "ULXmVSjyXaTq",
        "outputId": "f2fddf23-115b-4e6c-94ed-5c3748c33aab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-35f651ae50ce>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chat1_processed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chat1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text_lemmatize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chat2_processed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chat2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text_lemmatize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-35f651ae50ce>\u001b[0m in \u001b[0;36mpreprocess_text_lemmatize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_text_lemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "kG34yuf7aDLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Lemmatization instead of Stemming\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text_lemmatize(text):\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "data['chat1_processed'] = data['chat1'].apply(preprocess_text_lemmatize)\n",
        "data['chat2_processed'] = data['chat2'].apply(preprocess_text_lemmatize)\n",
        "\n",
        "\n",
        "# Using a more complex model like XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Extract features from the preprocessed text data\n",
        "X = vectorizer.fit_transform(data['chat1_processed'] + ' ' + data['chat2_processed'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a XGBClassifier model on the training set\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred_xgb))\n",
        "print('Precision:', precision_score(y_test, y_pred_xgb))\n",
        "print('Recall:', recall_score(y_test, y_pred_xgb))\n",
        "print('F1 score:', f1_score(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHua6mQpaviR",
        "outputId": "b2be6649-bbda-4de6-e529-10d5a0e5ab32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7583333333333333\n",
            "Precision: 0.7704918032786885\n",
            "Recall: 0.7580645161290323\n",
            "F1 score: 0.7642276422764228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/NLPFinalProject/Model Implementation/imbalancedtask1.csv')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text_lemmatize(text):\n",
        "    if isinstance(text, str):  # Check if text is a string\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "data['chat1_processed'] = data['chat1'].apply(preprocess_text_lemmatize)\n",
        "data['chat2_processed'] = data['chat2'].apply(preprocess_text_lemmatize)\n",
        "\n",
        "# Using a more complex model like XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Extract features from the preprocessed text data\n",
        "X = vectorizer.fit_transform(data['chat1_processed'] + ' ' + data['chat2_processed'])"
      ],
      "metadata": {
        "id": "-kPFrfGxhfLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8ed0e0-3955-4765-99e2-d5b98f2920c7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using SMOTE for oversampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Perform oversampling before splitting into training and test sets\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, data['label'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'subsample': [0.5,],\n",
        "    'colsample_bytree': [0.5],\n",
        "    'n_estimators' : [100],\n",
        "    'objective': ['binary:logistic']\n",
        "}\n",
        "\n",
        "# Initialize the classifier\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, scoring='f1', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters: \", best_params)\n",
        "\n",
        "# Train a XGBClassifier model on the training set with the best parameters\n",
        "xgb_best = XGBClassifier(**best_params)\n",
        "xgb_best.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_xgb = xgb_best.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred_xgb))\n",
        "print('Precision:', precision_score(y_test, y_pred_xgb))\n",
        "print('Recall:', recall_score(y_test, y_pred_xgb))\n",
        "print('F1 score:', f1_score(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks3-2km7b4ra",
        "outputId": "9f8f7137-0cda-4a35-e569-e9780cd6d9d5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best parameters:  {'colsample_bytree': 0.5, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 100, 'objective': 'binary:logistic', 'subsample': 0.5}\n",
            "Accuracy: 0.8924242424242425\n",
            "Precision: 0.922360248447205\n",
            "Recall: 0.8658892128279884\n",
            "F1 score: 0.8932330827067669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFsLKxrQhgQR",
        "outputId": "90295b67-d953-4405-d5ec-8e858986ccc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       False       0.86      0.92      0.89       317\n",
            "        True       0.92      0.87      0.89       343\n",
            "\n",
            "    accuracy                           0.89       660\n",
            "   macro avg       0.89      0.89      0.89       660\n",
            "weighted avg       0.89      0.89      0.89       660\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZOnU8KDrhhBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}